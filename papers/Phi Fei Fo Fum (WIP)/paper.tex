% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
  \setmainfont[]{Times New Roman}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Phi, Fei, Fo, Fum: Effect Sizes for Chi-squared Tests},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Phi, Fei, Fo, Fum: Effect Sizes for Chi-squared Tests}
\author{}
\date{\vspace{-2.5em}}

\begin{document}
\maketitle

\hypertarget{abstract}{%
\section{Abstract}\label{abstract}}

In both theoretical and applied research, it is often of interest to
assess the strength of an observed association. Existing guidelines also
frequently recommend going beyond null-hypothesis significance testing
and to report effect sizes and their confidence intervals. As such,
measures of effect sizes are increasingly reported, valued, and
understood. Beyond their value in shaping the interpretation of the
results from a given study, reporting effect sizes is critical for
meta-analyses, which rely on their aggregation. We here review the most
common effect sizes for analyses of categorical variables that use the
\(\chi^2\) (chi-square) statistic, and introduce a new effect size---פ
(Fei, pronounced /fej/ or ``fay''). We demonstrate the implementation of
these measures and their confidence intervals via the
\texttt{\{effectsize\}} package (Ben-Shachar, Lüdecke, \& Makowski,
2020) in the R programming language.

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Over the last two decades, there has been growing concerns about the
so-called replication crisis in psychology and other fields (Camerer et
al., 2018; Open Science Collaboration, 2015). As a result, the
scientific community has paid increasing attention to the issue of
replicability in science, as well as to to good research and statistical
practices.

In this context, many have highlighted the limitations of
null-hypothesis significance testing and called for more modern
approaches to statistics (Cumming, 2014). One such recommendation coming
for example from the New Statistics movement is to report effect sizes
and their corresponding confidence intervals, and to increasingly rely
on meta-analyses to increase confidence in those estimations. These
recommendations are meant to complement (or even replace, according to
some) null-hypothesis significance testing and would help transition
toward a ``cumulative quantitative discipline''.

These so-called ``New Statistics'' are synergistic because effect sizes
are not only useful for interpreting study results in themselves, but
also because they are necessary for meta-analyses, which aggregate
effect sizes and their confidence intervals to create a summary effect
size of its own Wiernik \& Dahlke (2020).

Unfortunately, popular software do not always offer the necessary
implementations of the specialized effect sizes necessary for a given
research design. In this paper, we review the most commonly used effect
sizes for analyses of categorical variables that use the \(\chi^2\)
(chi-square) test statistic, and introduce a new effect size---פ (Fei,
pronounced /fej/ or ``fay'').

Importantly, we offer researchers an applied walkthrough on how to use
these effect sizes in practice thanks to the \texttt{\{effectsize\}}
package (Ben-Shachar et al., 2020) in the R programming language (R Core
Team, 2023), which implements these measures and their confidence
intervals. We cover in turn tests of independence (\emph{φ}/phi,
Cramér's \emph{V}) and tests of goodness of fit (Cohen's \emph{w} and a
new proposed effect size, פ/Fei).

\hypertarget{tests-of-independence}{%
\section{Tests of Independence}\label{tests-of-independence}}

The \(\chi^2\) test of independence between two categorical variables
examines if the frequency distribution of one of the variables is
dependent on the other. That is, are the two variables correlated such
that, for example, members of group 1 on variable X are more likely to
be members of group A on variable Y, rather than evenly spread across Y
variable groups A and B. Formally, the test examines how likely the
observed conditional frequencies (cell frequencies) are under the null
hypotheses of independence. This is done by examining the degree the
observed cell frequencies deviate from the frequencies that would be
expected if the variables were indeed independent. The test statistic
for these tests is the \(\chi^2\), which is computed as:

\[
\chi^2 = \sum_{i=1}^{l\times k}{\frac{(O_i-E_i)^2}{E_i}}
\]

Where \(O_i\) are the \emph{observed} frequencies and \(E_i\) are the
frequencies \emph{expected} under independence, and \(l\) and \(k\) are
the number of rows and columns of the contingency table.

Instead of the deviations between the observed and expected frequencies,
we can write \(\chi^2\) in terms of observed and expected cell
\emph{probabilities} and the total sample size \(N\) (since \(p=k/N\)):

\[
\chi^2 = N\times\sum_{i=1}^{l\times k}{\frac{(p_{O_i}-p_{E_i})^2}{p_{E_i}}}
\]

Where \(p_{O_i}\) are the \emph{observed} cell probabilities and
\(p_{E_i}\) are the probabilities \emph{expected} under independence.

For example, which might ask of the probability of survival of the
sinking of the Titanic is dependant on the sex of the passenger:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(Titanic\_xtab }\OtherTok{\textless{}{-}} \FunctionTok{as.table}\NormalTok{(}\FunctionTok{apply}\NormalTok{(Titanic, }\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{), sum)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         Survived
## Sex        No  Yes
##   Male   1364  367
##   Female  126  344
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{chisq.test}\NormalTok{(Titanic\_xtab)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Pearson's Chi-squared test with Yates' continuity correction
## 
## data:  Titanic_xtab
## X-squared = 454.5, df = 1, p-value < 2.2e-16
\end{verbatim}

\hypertarget{phi}{%
\subsection{Phi}\label{phi}}

For a 2-by-2 contingency table analysis, like the one used above, the
\(\phi\) (\emph{phi}) coefficient is a correlation-like measure of
effect size indicating the strength of association between the two
binary variables. One way to compute this effect size is to re-code the
binary variables as dummy (0, 1) variables, and computing the Pearson
correlation between them:

\[
\phi = |r_{AB}|
\]

Another way to compute \(\phi\) is by using the \(\chi^2\) statistic:

\[
\phi = \sqrt{\frac{\chi^2}{N}} = \sqrt{\sum_{i=1}^{l\times k}{\frac{(p_{O_i}-p_{E_i})^2}{p_{E_i}}}}
\]

This value ranges between 0 (no association) and 1 (complete
dependence), and its values can be interpreted the same as Person's
correlation coefficient.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(effectsize)}
\FunctionTok{library}\NormalTok{(correlation)}

\FunctionTok{phi}\NormalTok{(Titanic\_xtab, }\AttributeTok{adjust =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Phi  |       95% CI
## -------------------
## 0.46 | [0.42, 1.00]
## 
## - One-sided CIs: upper bound fixed at [1.00].
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidyr}\SpecialCharTok{::}\FunctionTok{uncount}\NormalTok{(}\FunctionTok{as.data.frame}\NormalTok{(Titanic\_xtab), }\AttributeTok{weights =}\NormalTok{ Freq) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{transform}\NormalTok{(}\AttributeTok{Survived =}\NormalTok{ Survived }\SpecialCharTok{==} \StringTok{"Yes"}\NormalTok{,}
            \AttributeTok{Sex =}\NormalTok{ Sex }\SpecialCharTok{==} \StringTok{"Male"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{correlation}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # Correlation Matrix (pearson-method)
## 
## Parameter1 | Parameter2 |     r |         95% CI | t(2199) |         p
## ----------------------------------------------------------------------
## Sex        |   Survived | -0.46 | [-0.49, -0.42] |  -24.00 | < .001***
## 
## p-value adjustment method: Holm (1979)
## Observations: 2201
\end{verbatim}

\hypertarget{cramuxe9rs-v}{%
\subsection{\texorpdfstring{Cramér's
\emph{V}}{Cramér's V}}\label{cramuxe9rs-v}}

When the contingency table is larger than 2-by-2, using
\(\sqrt{\chi^2/N}\) can produce values larger than 1, and so loses its
interpretability as a correlation like effect size. Cramér showed
(Cramér, 1999) that while for 2-by-2 the maximal possible value of
\(\chi^2\) is \(N\), for larger tables the maximal possible value for
\(\chi^2\) is \(N\times (\text{min}(k,l)-1)\). Therefore, he suggested
the \(V\) effect size (also sometimes known as Cramér's phi and denoted
as \(\phi_{c}\)):

\[
\text{Cramer's } V = \sqrt{\frac{\chi^2}{N(\text{min}(k,l)-1)}}
\]

\(V\) is 1 when the columns are completely dependent on the rows, or the
row are completely dependent on the columns.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(Titanic\_xtab2 }\OtherTok{\textless{}{-}} \FunctionTok{as.table}\NormalTok{(}\FunctionTok{apply}\NormalTok{(Titanic, }\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{), sum)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       Survived
## Class   No Yes
##   1st  122 203
##   2nd  167 118
##   3rd  528 178
##   Crew 673 212
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cramers\_v}\NormalTok{(Titanic\_xtab2, }\AttributeTok{adjust =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Cramer's V |       95% CI
## -------------------------
## 0.29       | [0.26, 1.00]
## 
## - One-sided CIs: upper bound fixed at [1.00].
\end{verbatim}

Tschuprow (Tschuprow, 1939) devised an alternative value, at

\[
\text{Tschuprow's } t = \sqrt{\frac{\chi^2}{N\sqrt{(k-1)(l-1)}}}
\]

which is 1 only when the columns are completely dependent on the rows
\emph{and} the rows are completely dependent on the columns, which is
only possible when the contingency table is a square.

For example, in the following table, each row is dependent on the column
value; that is, if we know if the food is a soy, milk or meat product,
we also know if the food is vegan or not. However, the columns are
\emph{not} fully dependent on the rows: knowing the food is vegan tells
us the food is soy based, however knowing it is not vegan does not allow
us to classify the food - it can be either a milk product or a meat
product.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(}\StringTok{"food\_class"}\NormalTok{)}
\NormalTok{food\_class}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           Soy Milk Meat
## Vegan      47    0    0
## Not-Vegan   0   12   21
\end{verbatim}

Accordingly, in such a table, Cramer's \emph{V} will be 1, but
Tschuprow's \emph{t} will not be:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cramers\_v}\NormalTok{(food\_class, }\AttributeTok{adjust =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Cramer's V |       95% CI
## -------------------------
## 1.00       | [0.81, 1.00]
## 
## - One-sided CIs: upper bound fixed at [1.00].
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tschuprows\_t}\NormalTok{(food\_class)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Tschuprow's T |       95% CI
## ----------------------------
## 0.84          | [0.68, 1.00]
## 
## - One-sided CIs: upper bound fixed at [1.00].
\end{verbatim}

We can generalize both \(\phi\), \(V\), and \(T\) to:
\(\sqrt{\frac{\chi^2}{\chi^2_{\text{max}}}}\).

These coefficients can also be used for confusion matrices in the
context of assessing machine learning algorithms classification
abilities. In fact, a popular metric is the Matthews correlation
coefficient (MCC) for binary classifiers, which is often presented in
terms of true and false positives and negatives, is nothing more that
\(\phi\) (Chicco \& Jurman, 2020).

\hypertarget{goodness-of-fit}{%
\section{Goodness of Fit}\label{goodness-of-fit}}

These tests compare an observed distribution of a multinomial variable
to an expected distribution, using the same \(\chi^2\) statistic. Here
too we can compute an effect size as
\(\sqrt{\frac{\chi^2}{\chi^2_{\text{max}}}}\), all we need to find is
\(\chi^2_{\text{max}}\).

\hypertarget{cohens-w}{%
\subsection{\texorpdfstring{Cohen's
\emph{w}}{Cohen's w}}\label{cohens-w}}

Cohen (Cohen, 2013) defined an effect size---\emph{w}---for the goodness
of fit test:

\[
\text{Cohen's } w = \sqrt{\sum_{i=1}^{k}{\frac{(p_{O_i}-p_{E_i})^2}{p_{E_i}}}} = \sqrt{\frac{\chi^2}{N}}
\]

Thus, \(\chi^2_\text{max} = N\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(Titanic\_freq }\OtherTok{\textless{}{-}} \FunctionTok{as.table}\NormalTok{(}\FunctionTok{apply}\NormalTok{(Titanic, }\DecValTok{2}\NormalTok{, sum)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Male Female 
##   1731    470
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p\_E }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.5}\NormalTok{)}

\FunctionTok{cohens\_w}\NormalTok{(Titanic\_freq, }\AttributeTok{p =}\NormalTok{ p\_E)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Cohen's w |       95% CI
## ------------------------
## 0.57      | [0.54, 1.00]
## 
## - One-sided CIs: upper bound fixed at [1.00].
\end{verbatim}

Unfortunately, \emph{w} has an upper bound of 1 \emph{only} when the
variable is binomial (has two categories) and the expected distribution
is uniform (\(p = 1 - p = 0.5\)). If the distribution is none uniform
(Rosenberg, 2010) or if there are more than 2 classes (Johnston, Berry,
\& Mielke Jr, 2006), then \(\chi^2_\text{max} > N\), and so \emph{w} can
be larger than 1.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{O }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{90}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\NormalTok{p\_E }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{0.35}\NormalTok{, }\FloatTok{0.65}\NormalTok{)}
\FunctionTok{cohens\_w}\NormalTok{(O, }\AttributeTok{p =}\NormalTok{ p\_E)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Cohen's w |       95% CI
## ------------------------
## 1.15      | [0.99, 1.36]
## 
## - One-sided CIs: upper bound fixed at [1.36~].
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{O }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{80}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\NormalTok{p\_E }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(.}\DecValTok{25}\NormalTok{, .}\DecValTok{25}\NormalTok{, .}\DecValTok{25}\NormalTok{, .}\DecValTok{25}\NormalTok{)}
\FunctionTok{cohens\_w}\NormalTok{(O, }\AttributeTok{p =}\NormalTok{ p\_E)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Cohen's w |       95% CI
## ------------------------
## 1.05      | [0.88, 1.73]
## 
## - One-sided CIs: upper bound fixed at [1.73~].
\end{verbatim}

\hypertarget{fei}{%
\subsection{Fei}\label{fei}}

We present here a new effect size, פ (Fei, pronounced /fej/ or ``fay''),
which normalizes goodness-of-fit \(\chi^2\) by the proper
\(\chi^2_\text{max}\) for non-uniform and/or multinomial variables.

The largest deviation from the expected probability distribution would
occur when all observations are in the cell with the smallest expected
probability. That is:

\[
p_{O} = 
\begin{cases}
1, & \text{if } p_i = \text{min}(p) \\
0, & \text{Otherwise}
\end{cases}
\]

We can find \(\frac{(E_i-O_i)^2}{E_i}\) for each of these values:

\[
\frac{(p_{E}-p_{O})^2}{p_{E}} = 
\begin{cases}
\frac{(p_i-1)^2}{p_i} = \frac{(1-p_i)^2}{p_i}, & \text{if } p_{E} = \text{min}(p_{E}) \\
\frac{(p_i-0)^2}{p_i} = p_i, & \text{Otherwise}
\end{cases}
\]

Therefore,

\[
\begin{split}
\sum_{i=1}^{k}{\frac{(p_{O_i}-p_{E_i})^2}{p_{E_i}}} & = \sum_{i=1}^{k}{p_{E_i}} - \text{min}(p_{E}) + \frac{(1-\text{min}(p_{E}))^2}{\text{min}(p_{E})} \\
& = 1 - \text{min}(p_E) + \frac{(1-\text{min}(p_E))^2}{\text{min}(p_E)} \\
& = \frac{1-\text{min}(p_E)}{\text{min}(p_E)} \\
& = \frac{1}{\text{min}(p_E)} - 1
\end{split}
\]

And so,

\[
\begin{split}
\chi^2_\text{max} & = N \times \sum_{i=1}^{k}{\frac{(p_{O_i}-p_{E_i})^2}{p_{E_i}}} \\
 & = N \times (\frac{1}{\text{min}(p_E)} - 1)
\end{split}
\] Finally, an effect size can be derived as:

\[
\sqrt{\frac{\chi^2}{N \times (\frac{1}{\text{min}(p_E)} - 1)}}
\]

We call this effect size פ (Fei), which represents the voiceless
bilabial fricative in the Hebrew language, keeping in line with \(\phi\)
(which in modern Greek marks the same sound) and \(V\) (which in English
marks a voiced bilabial fricative; \(W\) being derived from the letter V
in modern Latin alphabet). פ will be 0 when the observed distribution
matches the expected one perfectly, and will be 1 when the observed
values are all of the same class---the one with the smallest expected
probability.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{O }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{90}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\NormalTok{p\_E }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{0.35}\NormalTok{, }\FloatTok{0.65}\NormalTok{)}
\FunctionTok{fei}\NormalTok{(O, }\AttributeTok{p =}\NormalTok{ p\_E)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Fei  |       95% CI
## -------------------
## 0.85 | [0.73, 1.00]
## 
## - Adjusted for uniform expected probabilities.
## - One-sided CIs: upper bound fixed at [1.00].
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{O }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{80}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\NormalTok{p\_E }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(.}\DecValTok{25}\NormalTok{, .}\DecValTok{25}\NormalTok{, .}\DecValTok{25}\NormalTok{, .}\DecValTok{25}\NormalTok{)}
\FunctionTok{fei}\NormalTok{(O, }\AttributeTok{p =}\NormalTok{ p\_E)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Fei  |       95% CI
## -------------------
## 0.60 | [0.51, 1.00]
## 
## - Adjusted for non-uniform expected probabilities.
## - One-sided CIs: upper bound fixed at [1.00].
\end{verbatim}

When there are only 2 cells with uniform expected probabilities (50\%),
this expression reduces to \(N\) and פ \(= w\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{O }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{90}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\NormalTok{p\_E }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.5}\NormalTok{)}

\FunctionTok{fei}\NormalTok{(O, }\AttributeTok{p =}\NormalTok{ p\_E)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Fei  |       95% CI
## -------------------
## 0.80 | [0.64, 1.00]
## 
## - One-sided CIs: upper bound fixed at [1.00].
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cohens\_w}\NormalTok{(O, }\AttributeTok{p =}\NormalTok{ p\_E)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Cohen's w |       95% CI
## ------------------------
## 0.80      | [0.64, 1.00]
## 
## - One-sided CIs: upper bound fixed at [1.00].
\end{verbatim}

\hypertarget{summary}{%
\section{Summary}\label{summary}}

Effect sizes are essential to interpret the magnitude of observed
effects, they are frequently required in scientific journals, and they
are are necessary for a cumulative quantitative science relying on
meta-analyses. In this paper, we have covered the mathematics and
implementation in R of four different effect sizes for analyses of
categorical variables that specifically use the \(\chi^2\) (chi-square)
statistic. Furthermore, with our proposal of the effect size פ (Fei), we
fill the missing effect size for all cases of a \(\chi^2\) test.

\hypertarget{references}{%
\section*{References}\label{references}}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-benshachar2020effectsize}{}}%
Ben-Shachar, M. S., Lüdecke, D., \& Makowski, D. (2020). {e}ffectsize:
Estimation of effect size indices and standardized parameters.
\emph{Journal of Open Source Software}, \emph{5}(56), 2815.
\url{https://doi.org/10.21105/joss.02815}

\leavevmode\vadjust pre{\hypertarget{ref-camerer2018evaluating}{}}%
Camerer, C. F., Dreber, A., Holzmeister, F., Ho, T.-H., Huber, J.,
Johannesson, M., \ldots{} Wu, H. (2018). Evaluating the replicability of
social science experiments in nature and science between 2010 and 2015.
\emph{Nature Human Behaviour}, \emph{2}, 637--644.
\url{https://doi.org/10.1038/s41562-018-0399-z}

\leavevmode\vadjust pre{\hypertarget{ref-chicco2020advantages}{}}%
Chicco, D., \& Jurman, G. (2020). The advantages of the matthews
correlation coefficient (MCC) over F1 score and accuracy in binary
classification evaluation. \emph{BMC Genomics}, \emph{21}, 1--13.

\leavevmode\vadjust pre{\hypertarget{ref-cohen2013statistical}{}}%
Cohen, J. (2013). \emph{Statistical power analysis for the behavioral
sciences}. Routledge.

\leavevmode\vadjust pre{\hypertarget{ref-cramer1999mathematical}{}}%
Cramér, H. (1999). \emph{Mathematical methods of statistics} (Vol. 43).
Princeton University Press.

\leavevmode\vadjust pre{\hypertarget{ref-cumming2014new}{}}%
Cumming, G. (2014). The new statistics: Why and how. \emph{Psychological
Science}, \emph{25}(1), 7--29.
\url{https://doi.org/10.1177/0956797613504966}

\leavevmode\vadjust pre{\hypertarget{ref-degeest2010impact}{}}%
DeGeest, D. S., \& Schmidt, F. L. (2010). The impact of research
synthesis methods on industrial--organizational psychology: The road
from pessimism to optimism about cumulative knowledge. \emph{Research
Synthesis Methods}, \emph{1}(3-4), 185--197.

\leavevmode\vadjust pre{\hypertarget{ref-johnston2006measures}{}}%
Johnston, J. E., Berry, K. J., \& Mielke Jr, P. W. (2006). Measures of
effect size for chi-squared and likelihood-ratio goodness-of-fit tests.
\emph{Perceptual and Motor Skills}, \emph{103}(2), 412--414.

\leavevmode\vadjust pre{\hypertarget{ref-OSC2015estimating}{}}%
Open Science Collaboration. (2015). Estimating the reproducibility of
psychological science. \emph{Science}, \emph{349}, aac4716.
\url{https://doi.org/10.1126/science.aac4716}

\leavevmode\vadjust pre{\hypertarget{ref-base2023}{}}%
R Core Team. (2023). \emph{R: A language and environment for statistical
computing}. Retrieved from \url{https://www.R-project.org/}

\leavevmode\vadjust pre{\hypertarget{ref-rosenberg2010generalized}{}}%
Rosenberg, M. S. (2010). A generalized formula for converting chi-square
tests to effect sizes for meta-analysis. \emph{PloS One}, \emph{5}(4),
e10059.

\leavevmode\vadjust pre{\hypertarget{ref-tschuprow1939principles}{}}%
Tschuprow, A. A. (1939). \emph{Principles of the mathematical theory of
correlation}. Hodge.

\leavevmode\vadjust pre{\hypertarget{ref-wiernik2020unbiased}{}}%
Wiernik, B. M., \& Dahlke, J. A. (2020). Obtaining unbiased results in
meta-analysis: The importance of correcting for statistical artifacts.
\emph{Advances in Methods and Practices in Psychological Science},
\emph{3}(1), 94--123.

\end{CSLReferences}

\end{document}
